from sklearn.utils import shuffle
from bs4 import BeautifulSoup 
import re
import nltk
nltk.download("stopwords")   
from nltk.corpus import stopwords 
from nltk.stem.porter import *
stemmer = PorterStemmer()
import pickle 
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.externals import joblib
import sklearn.preprocessing as pr

#prepare dataset///////////////////////////////////////////////////////
def prepare_imdb_data(data):
    """Prepare training and test sets from IMDb movie reviews."""
    data_train = data['train']['pos'] + data['train']['neg']
    data_test = data['test']['pos'] + data['test']['neg']
    labels_train = labels['train']['pos'] + labels['train']['neg']
    labels_test = labels['test']['pos'] + labels['test']['neg']
    
    data_train, labels_train = shuffle(data_train, labels_train)
    data_test, labels_test = shuffle(data_test, labels_test)
    # Return a unified training data, test data, training labels, test labets
    return data_train, data_test, labels_train, labels_test


data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)
print("IMDb reviews (combined): train = {}, test = {}".format(len(data_train), len(data_test)))

#preprocessing data////////////////////////////////////////////////////
def review_to_words(review):
    """Convert a raw review string into a sequence of words.""" 
    review = BeautifulSoup(review)
    review = re.sub(r"[^a-zA-Z0-9]", " ",review.get_text())
    words=review.lower().split()
    words = [stemmer.stem(w) for w in words if w not in stopwords.words('english')]
    return words

cache_dir = os.path.join("cache", "sentiment_analysis")  # where to store cache files
os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists

def preprocess_data(data_train, data_test, labels_train, labels_test,cache_dir=cache_dir, cache_file="preprocessed_data.pkl"):
    """Convert each review to words; read from cache if available."""
    cache_data = None
    if cache_file is not None:
        try:
            with open(os.path.join(cache_dir, cache_file), "rb") as f:
                cache_data = pickle.load(f)
            print("Read preprocessed data from cache file:", cache_file)
        except:
            pass  
        if cache_data is None:
        words_train = list(map(review_to_words, data_train))
        words_test = list(map(review_to_words, data_test))
        if cache_file is not None:
            cache_data = dict(words_train=words_train, words_test=words_test,
                              labels_train=labels_train, labels_test=labels_test)
            with open(os.path.join(cache_dir, cache_file), "wb") as f:
                pickle.dump(cache_data, f)
            print("Wrote preprocessed data to cache file:", cache_file)
    else:
        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],
                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])    
    return words_train, words_test, labels_train, labels_test

words_train, words_test, labels_train, labels_test = preprocess_data(data_train, data_test, labels_train, labels_test)

#extracting bag of words features/////////////////////////////////////////////////////////
def extract_BoW_features(words_train, words_test, vocabulary_size=5000,cache_dir=cache_dir, cache_file="bow_features.pkl"):
    """Extract Bag-of-Words for a given set of documents, already preprocessed into words."""
    cache_data = None
    if cache_file is not None:
        try:
            with open(os.path.join(cache_dir, cache_file), "rb") as f:
                cache_data = joblib.load(f)
            print("Read features from cache file:", cache_file)
        except:
            pass  
    if cache_data is None:
        vectorizer = CountVectorizer(max_features=vocabulary_size,preprocessor=lambda x: x, tokenizer=lambda x: x)
        features_train = vectorizer.fit_transform(words_train).toarray()
        features_test = vectorizer.fit_transform(words_test).toarray()
        if cache_file is not None:
            vocabulary = vectorizer.vocabulary_
            cache_data = dict(features_train=features_train, features_test=features_test,
                             vocabulary=vocabulary)
            with open(os.path.join(cache_dir, cache_file), "wb") as f:
                joblib.dump(cache_data, f)
            print("Wrote features to cache file:", cache_file)
    else:
        features_train, features_test, vocabulary = (cache_data['features_train'],
                cache_data['features_test'], cache_data['vocabulary'])
    return features_train, features_test, vocabulary

features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)

#normalize////////////////
features_train = pr.normalize(features_train, axis=1)
features_test = pr.normalize(features_test, axis=1)
